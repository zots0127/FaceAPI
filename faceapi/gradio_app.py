#!/usr/bin/env python3
"""
FaceAPI Gradio Webç•Œé¢
æä¾›ç›´è§‚çš„Webç•Œé¢è¿›è¡Œäººè„¸æ£€æµ‹å’Œå…³é”®ç‚¹æå–
"""

import gradio as gr
import cv2
import numpy as np
import json
import tempfile
import os
from typing import Tuple, List, Optional

try:
    from .core import MediaPipeFaceDetector, MultiYOLODetector
    from .utils import extract_face
    CORE_AVAILABLE = True
except ImportError:
    CORE_AVAILABLE = False

# åˆå§‹åŒ–æ£€æµ‹å™¨
if CORE_AVAILABLE:
    mediapipe_detector = MediaPipeFaceDetector()
    yolo_detector = MultiYOLODetector()
else:
    mediapipe_detector = None
    yolo_detector = None


def detect_faces_interface(image: np.ndarray, model: str, confidence: float,
                          enable_smart_crop: bool) -> Tuple[np.ndarray, str]:
    """
    äººè„¸æ£€æµ‹ç•Œé¢å‡½æ•°
    """
    if image is None:
        return None, "è¯·ä¸Šä¼ å›¾åƒ"

    if not CORE_AVAILABLE:
        return image, "âŒ æ ¸å¿ƒæ¨¡å—æœªæ­£ç¡®åŠ è½½"

    try:
        result_image = image.copy()
        faces = []

        # é€‰æ‹©æ£€æµ‹æ¨¡å‹
        if model == "MediaPipe":
            faces = mediapipe_detector.detect_faces(image)
        elif model.startswith("YOLO"):
            model_name = model.replace("YOLO ", "").lower()
            if model_name in yolo_detector.available_models:
                faces = yolo_detector.detect_faces(
                    image, model_name=model_name,
                    conf_threshold=confidence, enable_smart_crop=enable_smart_crop
                )
            else:
                return image, f"âŒ æ¨¡å‹ {model_name} ä¸å¯ç”¨"

        # ç»˜åˆ¶æ£€æµ‹ç»“æœ
        if faces:
            for i, face in enumerate(faces):
                x, y, w, h = face['bbox']
                conf = face['confidence']

                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                color = (0, 255, 0) if conf >= confidence else (0, 165, 255)
                cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f"Face {i+1}: {conf:.2f}"
                cv2.putText(result_image, label, (x, y - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            info = f"âœ… æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸\n"
            info += f"ä½¿ç”¨æ¨¡å‹: {model}\n"
            info += f"ç½®ä¿¡åº¦é˜ˆå€¼: {confidence}"
        else:
            info = "âŒ æœªæ£€æµ‹åˆ°äººè„¸"

        return result_image, info

    except Exception as e:
        return image, f"âŒ æ£€æµ‹å¤±è´¥: {str(e)}"


def landmarks_interface(image: np.ndarray) -> Tuple[np.ndarray, str]:
    """
    å…³é”®ç‚¹æ£€æµ‹ç•Œé¢å‡½æ•°
    """
    if image is None:
        return None, "è¯·ä¸Šä¼ å›¾åƒ"

    if not CORE_AVAILABLE or mediapipe_detector is None:
        return image, "âŒ MediaPipeæ£€æµ‹å™¨æœªåŠ è½½"

    try:
        result_image = image.copy()
        landmarks_data = mediapipe_detector.get_landmarks(image)

        if landmarks_data:
            # ç»˜åˆ¶å…³é”®ç‚¹
            for face_data in landmarks_data:
                landmarks = face_data['landmarks']

                # ç»˜åˆ¶æ‰€æœ‰å…³é”®ç‚¹
                for i, (x, y) in enumerate(landmarks):
                    cv2.circle(result_image, (x, y), 1, (0, 255, 0), -1)

                # ç»˜åˆ¶é¢éƒ¨è½®å»“
                # é¢éƒ¨è½®å»“ç‚¹ç´¢å¼•
                face_oval = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 340, 346, 347, 348, 349, 350, 451, 452, 453, 464, 435, 410, 287, 273, 335, 406, 313, 18, 83, 182, 106, 43, 57, 186, 92, 165, 167, 164, 393, 391, 322, 410, 287, 273, 335, 321, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]

                # ç»˜åˆ¶è½®å»“è¿çº¿
                for i in range(len(face_oval)):
                    start_point = landmarks[face_oval[i]]
                    end_point = landmarks[face_oval[(i + 1) % len(face_oval)]]
                    cv2.line(result_image, start_point, end_point, (255, 0, 0), 1)

            info = f"âœ… æå–åˆ° {len(landmarks_data)} ç»„å…³é”®ç‚¹\n"
            info += f"æ¯ç»„å…³é”®ç‚¹: 468ä¸ª\n"
            info += f"æ€»è®¡: {len(landmarks_data) * 468} ä¸ªç‚¹"
        else:
            info = "âŒ æœªæ£€æµ‹åˆ°äººè„¸å…³é”®ç‚¹"

        return result_image, info

    except Exception as e:
        return image, f"âŒ å…³é”®ç‚¹æå–å¤±è´¥: {str(e)}"


def extract_face_interface(image: np.ndarray, face_id: int, margin: int) -> Tuple[Optional[np.ndarray], str]:
    """
    äººè„¸æå–ç•Œé¢å‡½æ•°
    """
    if image is None:
        return None, "è¯·ä¸Šä¼ å›¾åƒ"

    if not CORE_AVAILABLE or mediapipe_detector is None:
        return image, "âŒ MediaPipeæ£€æµ‹å™¨æœªåŠ è½½"

    try:
        # æ£€æµ‹äººè„¸
        faces = mediapipe_detector.detect_faces(image)

        if not faces:
            return None, "âŒ æœªæ£€æµ‹åˆ°äººè„¸"

        if face_id >= len(faces):
            return None, f"âŒ äººè„¸IDè¶…å‡ºèŒƒå›´ (æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸)"

        # æå–æŒ‡å®šäººè„¸
        bbox = faces[face_id]['bbox']
        extracted_face = extract_face(image, bbox, margin)

        info = f"âœ… æˆåŠŸæå–äººè„¸ {face_id + 1}\n"
        info += f"è¾¹ç•Œæ¡†: {bbox}\n"
        info += f"è¾¹è·: {margin}px\n"
        info += f"æå–å°ºå¯¸: {extracted_face.shape[:2]}"

        return extracted_face, info

    except Exception as e:
        return None, f"âŒ äººè„¸æå–å¤±è´¥: {str(e)}"


def benchmark_interface(image: np.ndarray, confidence: float) -> str:
    """
    æ¨¡å‹åŸºå‡†æµ‹è¯•ç•Œé¢å‡½æ•°
    """
    if image is None:
        return "è¯·ä¸Šä¼ å›¾åƒ"

    if not CORE_AVAILABLE:
        return "âŒ æ ¸å¿ƒæ¨¡å—æœªæ­£ç¡®åŠ è½½"

    try:
        results = {}

        # MediaPipeæµ‹è¯•
        if mediapipe_detector:
            start_time = cv2.getTickCount()
            faces_mediapipe = mediapipe_detector.detect_faces(image)
            end_time = cv2.getTickCount()
            time_mediapipe = (end_time - start_time) / cv2.getTickFrequency() * 1000

            results['MediaPipe'] = {
                'faces': len(faces_mediapipe),
                'time': f"{time_mediapipe:.1f}ms"
            }

        # YOLOæ¨¡å‹æµ‹è¯•
        if yolo_detector and yolo_detector.available_models:
            for model_name in yolo_detector.available_models[:3]:  # åªæµ‹è¯•å‰3ä¸ªæ¨¡å‹
                start_time = cv2.getTickCount()
                faces_yolo = yolo_detector.detect_faces(
                    image, model_name=model_name, conf_threshold=confidence
                )
                end_time = cv2.getTickCount()
                time_yolo = (end_time - start_time) / cv2.getTickFrequency() * 1000

                results[f'YOLO {model_name}'] = {
                    'faces': len(faces_yolo),
                    'time': f"{time_yolo:.1f}ms"
                }

        # æ ¼å¼åŒ–ç»“æœ
        result_text = "ğŸ åŸºå‡†æµ‹è¯•ç»“æœ\n" + "="*30 + "\n\n"

        for model, data in results.items():
            result_text += f"ğŸ¤– {model}:\n"
            result_text += f"   æ£€æµ‹äººè„¸: {data['faces']} ä¸ª\n"
            result_text += f"   å¤„ç†æ—¶é—´: {data['time']}\n\n"

        return result_text

    except Exception as e:
        return f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}"


def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""

    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available_models = ["MediaPipe"]
    if CORE_AVAILABLE and yolo_detector:
        for model in yolo_detector.available_models:
            available_models.append(f"YOLO {model.upper()}")

    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="FaceAPI - æ™ºèƒ½äººè„¸æ£€æµ‹", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¯ FaceAPI - æ™ºèƒ½äººè„¸æ£€æµ‹

        åŸºäº MediaPipe + YOLO çš„é«˜æ€§èƒ½äººè„¸æ£€æµ‹ç³»ç»Ÿï¼Œæ”¯æŒå¤šæ¨¡å‹é€‰æ‹©å’Œæ™ºèƒ½é‡å è£å‰ªæŠ€æœ¯ã€‚
        """)

        with gr.Tabs():
            # äººè„¸æ£€æµ‹æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ” äººè„¸æ£€æµ‹"):
                with gr.Row():
                    with gr.Column():
                        input_image = gr.Image(type="numpy", label="è¾“å…¥å›¾åƒ")

                        with gr.Row():
                            model_choice = gr.Dropdown(
                                choices=available_models,
                                value="MediaPipe",
                                label="æ£€æµ‹æ¨¡å‹"
                            )
                            confidence_slider = gr.Slider(
                                minimum=0.1, maximum=1.0, value=0.5, step=0.05,
                                label="ç½®ä¿¡åº¦é˜ˆå€¼"
                            )

                        smart_crop_checkbox = gr.Checkbox(
                            label="å¯ç”¨æ™ºèƒ½è£å‰ª (é€‚ç”¨äºå¤§å›¾åƒ)",
                            value=True
                        )

                        detect_btn = gr.Button("ğŸ” æ£€æµ‹äººè„¸", variant="primary")

                    with gr.Column():
                        output_image = gr.Image(type="numpy", label="æ£€æµ‹ç»“æœ")
                        detect_info = gr.Textbox(label="æ£€æµ‹ä¿¡æ¯", lines=3)

                detect_btn.click(
                    fn=detect_faces_interface,
                    inputs=[input_image, model_choice, confidence_slider, smart_crop_checkbox],
                    outputs=[output_image, detect_info]
                )

            # å…³é”®ç‚¹æ£€æµ‹æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ å…³é”®ç‚¹æ£€æµ‹"):
                with gr.Row():
                    with gr.Column():
                        landmarks_input = gr.Image(type="numpy", label="è¾“å…¥å›¾åƒ")
                        landmarks_btn = gr.Button("ğŸ“ æå–å…³é”®ç‚¹", variant="primary")

                    with gr.Column():
                        landmarks_output = gr.Image(type="numpy", label="å…³é”®ç‚¹å¯è§†åŒ–")
                        landmarks_info = gr.Textbox(label="å…³é”®ç‚¹ä¿¡æ¯", lines=3)

                landmarks_btn.click(
                    fn=landmarks_interface,
                    inputs=[landmarks_input],
                    outputs=[landmarks_output, landmarks_info]
                )

            # äººè„¸æå–æ ‡ç­¾é¡µ
            with gr.TabItem("âœ‚ï¸ äººè„¸æå–"):
                with gr.Row():
                    with gr.Column():
                        extract_input = gr.Image(type="numpy", label="è¾“å…¥å›¾åƒ")

                        with gr.Row():
                            face_id_slider = gr.Slider(
                                minimum=0, maximum=10, value=0, step=1,
                                label="äººè„¸ID"
                            )
                            margin_slider = gr.Slider(
                                minimum=0, maximum=100, value=20, step=5,
                                label="è¾¹è· (åƒç´ )"
                            )

                        extract_btn = gr.Button("âœ‚ï¸ æå–äººè„¸", variant="primary")

                    with gr.Column():
                        extract_output = gr.Image(type="numpy", label="æå–çš„äººè„¸")
                        extract_info = gr.Textbox(label="æå–ä¿¡æ¯", lines=4)

                extract_btn.click(
                    fn=extract_face_interface,
                    inputs=[extract_input, face_id_slider, margin_slider],
                    outputs=[extract_output, extract_info]
                )

            # åŸºå‡†æµ‹è¯•æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ åŸºå‡†æµ‹è¯•"):
                with gr.Row():
                    with gr.Column():
                        benchmark_input = gr.Image(type="numpy", label="æµ‹è¯•å›¾åƒ")
                        benchmark_confidence = gr.Slider(
                            minimum=0.1, maximum=1.0, value=0.5, step=0.05,
                            label="ç½®ä¿¡åº¦é˜ˆå€¼"
                        )
                        benchmark_btn = gr.Button("ğŸ å¼€å§‹æµ‹è¯•", variant="primary")

                    with gr.Column():
                        benchmark_output = gr.Textbox(
                            label="æµ‹è¯•ç»“æœ",
                            lines=15,
                            max_lines=20
                        )

                benchmark_btn.click(
                    fn=benchmark_interface,
                    inputs=[benchmark_input, benchmark_confidence],
                    outputs=[benchmark_output]
                )

        # åº•éƒ¨ä¿¡æ¯
        gr.Markdown("""
        ---
        ### ğŸ“‹ ä½¿ç”¨è¯´æ˜
        1. **äººè„¸æ£€æµ‹**: é€‰æ‹©æ¨¡å‹å¹¶è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œç‚¹å‡»æ£€æµ‹æŒ‰é’®
        2. **å…³é”®ç‚¹æ£€æµ‹**: æå–468ä¸ªé¢éƒ¨å…³é”®ç‚¹å¹¶å¯è§†åŒ–
        3. **äººè„¸æå–**: é€‰æ‹©ç‰¹å®šäººè„¸IDå¹¶è°ƒæ•´è¾¹è·è¿›è¡Œæå–
        4. **åŸºå‡†æµ‹è¯•**: å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ€§èƒ½è¡¨ç°

        ### ğŸš€ æŠ€æœ¯ç‰¹æ€§
        - ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: MediaPipe + 6ä¸ªYOLOæ¨¡å‹
        - ğŸ“ **468å…³é”®ç‚¹**: ç²¾ç¡®çš„é¢éƒ¨ç‰¹å¾ç‚¹æ£€æµ‹
        - âœ‚ï¸ **æ™ºèƒ½è£å‰ª**: å¤§å›¾åƒè‡ªåŠ¨åˆ†å—å¤„ç†
        - âš¡ **é«˜æ€§èƒ½**: ä¼˜åŒ–çš„æ¨ç†é€Ÿåº¦
        """)

    return demo


def launch(host: str = "0.0.0.0", port: int = 7860, share: bool = False, debug: bool = False):
    """å¯åŠ¨Gradioåº”ç”¨"""
    if not CORE_AVAILABLE:
        print("âŒ æ ¸å¿ƒæ¨¡å—æœªæ­£ç¡®åŠ è½½ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
        return

    demo = create_interface()
    demo.launch(
        server_name=host,
        server_port=port,
        share=share,
        debug=debug,
        show_error=True
    )


def main():
    """Gradioåº”ç”¨å…¥å£ç‚¹"""
    launch()


if __name__ == "__main__":
    main()